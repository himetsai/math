\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers}{natbib}
% before loading neurips_2018

% ready for submission
% \usepackage{neurips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage[final]{neurips_2018}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{multirow}       % multirow cells in tables
\usepackage{amsmath}        % mathematical environments and commands
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsthm}
\usepackage{graphicx}       % include graphics

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]

\title{Smoothness is All You Need: Examining the Role of Smooth Activations in Efficient Robustness Trainings}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Candidate Number: idk
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Robustness training is notorious for its computational cost. To address this, one-step training methods like Input Gradient Regularization (IGR) and Fast Gradient Sign Method (FGSM) have been proposed, but their effectiveness has been inferior to multi-step methods, or even significantly compromised when enduring stronger attacks. In this project, we investigate the mathematical background of IGR and FGSM and how smooth activations could mitigate their limitations.
\end{abstract}

\section{Introduction}

Deep neural networks are very brittle to adversarial attacks; imperceptible perturbations to inputs can cause catastrophic misclassification \cite{szegedy2014intriguingpropertiesneuralnetworks}. Extensive research has been devoted to build robustness against adversarial attacks, with Adversarial Training via Projected Gradient Descent (PGD) \cite{madry2019deeplearningmodelsresistant} emerging as the most effective defense to date. While effective, Adversarial Training requires significant computational resources, as it involves iteratively generating strong adversarial examples during training, increasing training time by an order of magnitude compared to standard training. Consequently, one-step methods such as Input Gradient Regularization (IGR) and the Fast Gradient Sign Method (FGSM)  were proposed as efficient alternatives. 

Although both methods fundamentally rely on a first-order approximation of the robust optimization framework, they have historically been viewed as distinct optimization strategies. In particular, FGSM has been considered data augmentation, whereas IGR as a regularization technique. Consequently, they are often analyzed separately with distinct failure modes. IGR often leads to inferior performance \cite{ross2017improvingadversarialrobustnessinterpretability, seck2019l1normdoublebackpropagation, papernot2017practicalblackboxattacksmachine}, while FGSM suffers from catastrophic overfitting \cite{kang2021understandingcatastrophicoverfittingadversarial}, where the model becomes robust to the FGSM attack but performs disastrously (often $0\%$ robustness) against other types of attacks.

Recent work by Rodríguez-Muñoz et al. \cite{rodríguezmuñoz2024characterizingmodelrobustnessnatural} and Xie et al. \cite{xie2021smoothadversarialtraining} demonstrated respectively that IGR and FGSM can yield competitive results when using smooth activation functions like GeLU. However, their studies were mostly empirical and limited to IGR and activations with similar characteristics (GeLU and SiLU), leaving the broader role of smoothness in efficient training unexplored and the theoretical justification for their success unclear.

In this project, we propose a unified theoretical view of IGR and FGSM based on the local robustness bound established in \cite{finlay2019scaleableinputgradientregularization} and explain how the smoothness of activations impact the performance of both methods. By extending the analysis to Softplus and PReLU, we confirm that smoothness is the key to success and show that it effectively mitigates catastrophic overfitting in FGSM, allowing it to achieve robustness comparable to IGR.

\section{Mathematical Background}

In this section, we discuss the theoretical foundations of robustness training and propose a unified view of two historically distinct methods: FGSM-based training and Input Gradient Regularization (IGR). We begin by defining the robust optimization framework and the PGD attack introduced by Madry et al. \cite{madry2019deeplearningmodelsresistant}, as well as its relation to FGSM. We then show how IGR emerges as a first-order Taylor approximation of this framework using the derivations from Simon-Gabriel et al. \cite{simongabriel2019firstorderadversarialvulnerabilityneural}. Finally, we address the historical ineffectiveness of IGR and FGSM-based training by proposing a novel unified theoretical view of these two methods. In particular, we use the local robustness bound from Finlay and Oberman \cite{finlay2019scaleableinputgradientregularization} to decompose the FGSM objective, revealing exactly how it relates to IGR and why non-smooth activations allow models to invalidate the training process.

\subsection{The Robust Optimization Framework and Projective Gradient Descent}

Standard deep learning training attempts to find the model parameters $\theta$ that minimize the risk $\mathbb{E}_{(x, y) \sim \mathcal{D}}[\mathcal{L}(x, y; \theta)]$ over a data distribution $\mathcal{D}$. However, this approach yields models vulnerable to adversarial examples, where an input $x$ is susceptible to some small perturbation $\delta$ such that $x + \delta$ is misclassified. To address this, Madry et al. \cite{madry2019deeplearningmodelsresistant} proposed a general adversarial training objective, which is formulated as a saddle point problem:
\begin{equation}
  \min_{\theta} \mathbb{E}_{(x, y) \sim \mathcal{D}}[\max_{\delta \in \mathcal{S}} \mathcal{L}(x + \delta, y; \theta)].
  \label{eq:minimax}
\end{equation}
Here, $\mathcal{S}$ represents the set of allowed perturbations, typically an $\ell_p$-norm ball $\mathcal{S} = \{ \delta \in \mathbb{R}^d \mid \|\delta\|_p \leq \epsilon \}$. The rest of this project will focus on the standard $\ell_\infty$-bounded perturbations. 

The saddle point problem formulation in \eqref{eq:minimax} can be viewed as a battle between an adversary and a defender, where the defender seeks to find the best model parameters $\theta$ that minimizes the the maximum adversarial loss $\delta$. The inner maximization problem is often non-concave and difficult to solve exactly. The famous Fast Gradient Sign Method (FGSM) proposed by Goodfellow et al. \cite{goodfellow2015explainingharnessingadversarialexamples} can be interpreted as a simple one-step linearization of the inner maximization: By the first-order Taylor expansion,
\begin{equation}
  \mathcal{L}(x + \delta, y; \theta) \approx \mathcal{L}(x, y; \theta) + \delta^T\nabla_x \mathcal{L}(x, y; \theta),
  \label{eq:taylor}
\end{equation}
and so the FGSM attack is given by
\begin{equation}
  x^{\text{adv}} = x + \epsilon \cdot \operatorname{sign}(\nabla_x \mathcal{L}(x, y; \theta)) = x + \operatorname*{arg \, max}_{\|\delta\|_{\infty} \leq \epsilon} \delta^T\nabla_x \mathcal{L}(x, y; \theta). 
  \label{eq:fgsm}
\end{equation}
While computationally cheap, FGSM relies on the assumption that the loss surface is locally linear around $x$, which can be drasticaly far from the actual loss landscape and leads to the aforementioned catastrophic overfitting \cite{kang2021understandingcatastrophicoverfittingadversarial}.

Madry et al. demonstrated that projected gradient descent (PGD) is a more powerful multi-step variant of FGSM, which iteratively applies FGSM then projects the result back into the allowed perturbation set $\mathcal{S}$:
\[
  x^{(t+1)} = \Pi_{x + \mathcal{S}}(x^{(t)} + \alpha \operatorname{sign}(\nabla_x \mathcal{L}(x, y; \theta))).
\]
PGD is then applied to adversarial training framework, which replaces the natural input $x$ with the PGD generated adversarial example $x^{\text{adv}}$ during the outer minimization step in \eqref{eq:minimax}. While effective, this requires multiple gradient calculations per training step, increasing training time by an order of magnitude compared to standard training.

\subsection{Input Gradient Regularization as an Alternative}

In contrast to the high training cost of the multi-step Adversarial Training, IGR was proposed as a more efficient alternative that directly penalizes the input gradient norm \cite{7373334, ross2017improvingadversarialrobustnessinterpretability, ororbia2016unifyingadversarialtrainingalgorithms}. The heuristic motivation is that inducing local smoothness by forcing loss gradients to be small should make the model less sensitive to input perturbations. Consider the inner maximization problem for a perturbation $\delta$ bounded by $\|\delta\|_{\infty} \leq \epsilon$. Assuming the loss function $\mathcal{L}$ is differentiable with respect to the input $x$, it is shown in \cite{simongabriel2019firstorderadversarialvulnerabilityneural} that for small $\epsilon$, 
\[
  \max_{\|\delta\|_{\infty} \leq \epsilon} \mathcal{L}(x + \delta, y; \theta) \approx \mathcal{L}(x, y; \theta) + \epsilon\|\nabla_x \mathcal{L}(x, y; \theta)\|_{1}.
\]
This leads directly to the IGR objective, which effectively replaces the inner maximization problem in \eqref{eq:minimax} with a first-order approximation:
\begin{equation}
  \min_{\theta} \mathbb{E}_{(x, y) \sim \mathcal{D}}[\mathcal{L}(x, y; \theta) + \lambda\|\nabla_x \mathcal{L}(x, y; \theta)\|_1],
  \label{eq:igr}
\end{equation}
where $\lambda$ is a hyperparameter governing the strength of the regularization. 

However, formulating IGR merely as an approximation of the minimax problem ignores the complexity of the loss landscape. If the approximation error is large, minimizing the gradient norm may not result in true robustness. This is the reason why $\lambda$ is introduced in the IGR objective instead of directly using the attack strength $\epsilon$. To mathematically justify this, we examine the theoretical bound of robustness.

\subsection{The Theoretical Bound of Robustness}

The validity of IGR is theoretically grounded in \cite{finlay2019scaleableinputgradientregularization}, where Finlay and Oberman establish a lower bound on the size of an adversarial perturbation $\delta$ required to alter the class prediction based on local gradient information. At input $x$ and perturbation $\delta$, denote the first-order approximation error of the loss function as
\begin{equation}
  R(x, \delta) = \mathcal{L}(x + \delta) - [\mathcal{L}(x) + \langle \nabla_x \mathcal{L}(x), \delta \rangle].
  \label{eq:remainder}
\end{equation}
Then the first-order approximation error is bounded above by
\[
  \omega(\epsilon) = \sup_{x, \|\delta\| \leq \epsilon} R(x, \delta).
\]
We call $\omega(\epsilon)$ the modulus of continuity of the loss function and note that $\omega(\epsilon) \geq R(x, 0) = 0$.

\begin{proposition}[Finlay and Oberman \cite{finlay2019scaleableinputgradientregularization}]
  Let $\mathcal{L}(x)$ be a loss function and $\mathcal{L}_0$ be such that the model is correct whenever $\mathcal{L}(x) \leq \mathcal{L}_0$. Then the minimum magnitude of perturbation $\delta$ necessary to adversarially perturb an input $x$ is bounded below by $\epsilon$ if
  \begin{equation}
    \frac{\mathcal{L}_0 - \mathcal{L}(x) - \omega(\epsilon)}{\|\nabla_x \mathcal{L}(x)\|_1} \geq \epsilon. \label{eq:finlay_bound}
  \end{equation}
\end{proposition}
This bound yields three sufficient conditions for robustness: (i) large loss gap $\mathcal{L}_0 - \mathcal{L}(x)$, (ii) small first-order approximation error $\omega(\epsilon)$, and (iii) small input gradient norm $\|\nabla_x \mathcal{L}(x)\|_1$. This give a justification for IGR's effectiveness, as its objective function \eqref{eq:igr} directly targets condition (iii). However, this bound also highlights that minimizing the gradient norm is necessary but not sufficient. If the linearization error $\omega(\epsilon)$ is large, the lower bound on $\|\delta\|$ can remain small despite the gradient norm $\|\nabla_x \mathcal{L}(x)\|_1$ being small. This also justifies the use of $\lambda$ in \eqref{eq:igr}, as setting $\lambda = \epsilon$  is assuming the loss function is locally linear and ignoring the $\omega(\epsilon)$ term. Thus we scale $\lambda$ up to absorb the curvature of the loss function \cite{finlay2019scaleableinputgradientregularization}.

\subsection{Smoothness is What You Need}

Historically, IGR has struggled to achieve competitive robustness compared to PGD adversarial training \cite{seck2019l1normdoublebackpropagation}. This failure is closely linked to the first-order approximation error term $\omega(\epsilon)$ in \eqref{eq:finlay_bound} and how it is handled in non-smooth networks like ReLU. 

For non-smooth networks, the $\omega(\epsilon)$ can be large even for small $\epsilon$ as the local curvature may be unbounded, contradicting the requirement for robustness given by the robustness bound \eqref{eq:finlay_bound}. This mathematically explains the phenomenon of gradient masking \cite{papernot2017practicalblackboxattacksmachine}, where the model keeps the gradient norm small but does not provide robustness against other types of attacks. Finlay and Oberman did not directly resolve this issue in \cite{finlay2019scaleableinputgradientregularization} but offered a workaround using finite differences to estimate the gradient norm and avoided the calculation of second-order derivatives.

In recent work by Rodríguez-Muñoz et al. \cite{rodríguezmuñoz2024characterizingmodelrobustnessnatural} and Xie et al. \cite{xie2021smoothadversarialtraining}, it was shown that using smooth activation functions like GeLU and SiLU can significantly improve IGR's robustness. In particular, Rodríguez-Muñoz et al. showed replacing the non-smooth ReLU with the smooth GeLU and SiLU activation function achieves $> 90\%$ of robustness when compared to PGD adversarial training while using merely $63\%$ of the computing cost. Xie et al. discribed this as a better gradient quality, but we may further elaborate this success as smooth activations bounding the local first-order approximation error $R(x, \delta)$, which in turn bounds the modulus of continuity $\omega(\epsilon)$. This ensures condition (ii) in \eqref{eq:finlay_bound} is satisfied. While Rodríguez-Muñoz et al. demonstrated empirical evidence for the effectiveness of smooth activations, they only examined two smooth activation functions of extremely similar characteristics (GeLU and SiLU). We will experiment with other activation functions of different characteristics (e.g. Softplus, PReLU) to further isolate the effect of smoothness on IGR's performance and empirically verify if smoothness is indeed the key to IGR's success.

\subsection{IGR vs FGSM}

As mentioned in the previous sections, IGR and FGSM are both essentially first-order approximations of the inner maximization problem in \eqref{eq:minimax}. So what exactly is the difference between IGR and FGSM-based training? Why is FGSM-based training subjected to catastrophic overfitting but not IGR? In this section, we propose (to the best of our knowledge) a novel unified theoretical view of these two methods using the local robustness bound \eqref{eq:finlay_bound}. 

The phenomenon of catastrophic overfitting in FGSM-based training has been extensively studied, with numerous works identifying the breakdown of local linearity as the primary cause \cite{wong2020fastbetterfreerevisiting, andriushchenko2020understandingimprovingfastadversarial, kim2020understandingcatastrophicoverfittingsinglestep,rocamora2024efficientlocallinearityregularization}. To mitigate this, the dominant approach has been to introduce regularization terms that penalize curvature or gradient misalignment, such as GradAlign \cite{andriushchenko2020understandingimprovingfastadversarial}, Local Linearization Regularization \cite{qin2019adversarialrobustnesslocallinearization, rocamora2024efficientlocallinearityregularization}, and CURE \cite{moosavidezfooli2018robustnesscurvatureregularizationvice}. While these methods are effective, they treat the FGSM-based training and IGR as distinct optimization goals. 

Simon-Gabriel et al. \cite{simongabriel2019firstorderadversarialvulnerabilityneural} noted a link between the two methods by showing that FGSM-based training is a data augmentation technique that accounts for additional points $x + \delta$ perturbed by $\epsilon$-sized FGSM attacks $\delta = \epsilon \cdot \operatorname{sign}(\nabla_x \mathcal{L}(x))$, which alters the training loss function to
\begin{equation}
  \tilde{\mathcal{L}_{\epsilon}}(x, y; \theta) = \mathcal{L}(x, y; \theta) + \mathcal{L}(x + \delta, y; \theta).
  \label{eq:fgsm_loss_naive}
\end{equation}
On the other hand, IGR is simply adding a regularization term to the loss function to penalize the input gradient norm $\|\nabla_x \mathcal{L}(x)\|_1$. However, this comparison is insufficient to explain why FGSM-based training leads to catastrophic overfitting while IGR remains stable. Here is where we fill in the theoretical gap: if we introduce the approximation error term $R(x, \delta)$ defined in \eqref{eq:remainder}, we get
\begin{equation}
  \tilde{\mathcal{L}_{\epsilon}}(x, y; \theta) = \mathcal{L}(x, y; \theta) + \epsilon \|\nabla_x \mathcal{L}(x)\|_1 + R(x, \delta). \label{eq:fgsm_loss}
\end{equation}
Notice that this is essentially the same as the IGR loss function \eqref{eq:igr} when $\lambda = \epsilon$, with the critical difference being the addition of the approximation error term $R(x, \delta)$. This allows the model to ``cheat'' in FGSM-based training. Instead of minimizing the gradient norm $\|\nabla_x \mathcal{L}(x)\|_1$, the model may resort to minimizing $R(x, \delta)$ to be large and negative, contradicting the necessary condition (iii) for robustness given by the Finlay-Oberman bound.

By making $R(x, \delta)$ negative and $|R(x, \delta)|$ large, the model essentially invalidates the first-order approximation which FGSM relies on. This gives a mathematical explanation for why FGSM-based training leads to catastrophic overfitting. We suspect that this phenomenon is especially pronounced for non-smooth networks like ReLU, where the $R(x, \delta)$ term can be unbounded for the model to exploit. However, if we use smooth activation functions like GeLU, the term $R(x, \delta)$ is bounded and the loss function \eqref{eq:fgsm_loss} resembles the loss function of IGR \eqref{eq:igr}. Additionally, $R(x, \delta)$ being locally bounded suggests that the modulus of continuity $\omega(\epsilon)$ is also constrained, which aligns with a necessary condition for robustness given by \eqref{eq:finlay_bound}. Therefore, we suspect that FGSM-based training should achieve comparable robustness to IGR. This theoretical view is consistent with the experimental results in \cite{xie2021smoothadversarialtraining}, and we will also empirically verify FGSM-based training's failure modes and compare its performance with IGR in the next chapter.

\section{Experiments}

In this section, we provide empirical verification for the our unified theoretical view of IGR and FGSM as well as the effect of smooth activations on both methods. We train PreActResNet18 models on the CIFAR-10 dataset for IGR and FGSM with attack strength $\epsilon=8/255$. For IGR training, we follow the setup and adapt the codes from \cite{rodríguezmuñoz2024characterizingmodelrobustnessnatural}, which uses the following objective:
\[
  \mathcal{L}(x, y; \theta) = \lambda_{\text{CE}} \mathcal{L}_{\text{CE}}(x, y; \theta) + \lambda_{\text{GN}}\frac{\epsilon}{\sigma} \|\nabla_x \mathcal{L}_{\text{CE}}(x, y; \theta)\|_1,
\]
where $\mathcal{L}_{\text{CE}}$ is the cross entropy loss, $\lambda_{\text{CE}} = 0.8$ and $\lambda_{\text{GN}} = 1.2$ are weighting hyperparameters, $\epsilon = \frac{8}{255}$ is the adversarial strength, and $\sigma = 0.2023$ is the standard deviation used for normalization on CIFAR-10. For FGSM training, we use the objective defined in\eqref{eq:fgsm_loss_naive} and use the cross entropy loss. Both training objectives are trained for 100 epochs instead of 300 as in \cite{rodríguezmuñoz2024characterizingmodelrobustnessnatural}. The evaluation metric also follows \cite{rodríguezmuñoz2024characterizingmodelrobustnessnatural}, where we measure the clean accuracy, accuracy against PGD-50, and accuracy against AutoAttack \cite{croce2020reliableevaluationadversarialrobustness}, the current state-of-the-art ensemble of parameter-free attacks (APGD-CE, APGD-T, FAB-T, Square) for robustness.

A key novelty of our experimental design is the selection of activation functions. While \cite{rodríguezmuñoz2024characterizingmodelrobustnessnatural, xie2021smoothadversarialtraining} primarily compared ReLU against GELU or SiLU, which share very similar characteristics, we extend this analysis to include Softplus and PReLU to further isolate the effect of smoothness on IGR's performance. In contrast to GeLU and SiLU, Softplus is monotonic and essentially a smooth approximation of the ReLU, whereas PReLU is a parametric piecewise linear activation that is non-smooth. We also showcase the effect of smooth activations on FGSM-based training by plotting $R(x, \delta)$ defined in \eqref{eq:remainder} and $\|\nabla_x \mathcal{L}(x)\|_1$ to epoch graphs for ReLU and GeLU. A similar visualization for ReLU is done in \cite{rocamora2024efficientlocallinearityregularization}, but (to the best of our knowledge) its comparison to GeLU is novel.

\begin{table}[t]
  \centering
  \begin{tabular}{cc|ccc}
    \toprule
    \multicolumn{2}{c|}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{Accuracy (\%)}} \\ 
    \cmidrule(r){1-2} \cmidrule(l){3-5} 
    Training & Activation & Clean & PGD-50 & AutoAttack-$L_\infty$ \\ 
    \midrule
    \multirow{4}{*}{IGR} 
    & ReLU & 61.10 & 38.80 & 32.20 \\
    & PReLU & 58.80 & 37.60 & 30.90 \\
    & GELU & 82.50 & 44.40 & 39.30 \\
    & Softplus & 77.20 & 44.90 & 37.10 \\ 
    \midrule
    \multirow{2}{*}{FGSM} 
    & ReLU & 85.60 & 0.00 & 0.00 \\
    & GELU & 78.80 & 47.30 & 41.90 \\ 
    \bottomrule
  \end{tabular}
  \vspace{1em}
  \caption{Accuracy results for IGR and FGSM with 100 epochs of training.}
  \label{tab:main_results}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\textwidth]{images/fgsm.png}
  \caption{FGSM-based training performance for ReLU and GeLU activations. The plots from left to right are the clean accuracy, FGSM accuracy, approximation error $R(x, \delta)$, and gradient norm $\|\nabla_x \mathcal{L}(x)\|_1$ to epoch graph.}
  \label{fig:fgsm}
\end{figure}

It is clear in Table \ref{tab:main_results} that both methods achieve significant improvements by adopting smooth activation functions. Softplus's close performance to GELU in IGR suggests that monotonicity is not a major factor in IGR's success. PReLU also fails to achieve better result than ReLU despite being a generalization of ReLU. This indicates that that smoothness almost everywhere is insufficient and even a slight jump in the gradient could significatly reduce robustness. Both comparison yields further evidence that smooth activations are crucial for IGR's success. Meanwhile, FGSM achieves comparable robustness to IGR when using GeLU, confirming our guess earlier. Furthermore, our experiment perfectly captures the phenomenon of catastrophic overfitting and how the model ``cheats'' in FGSM-based training when using ReLU. As shown in Figure \ref{fig:fgsm}, the FGSM accuracy jumps from $50\%$ to near $100\%$ while $R(x, \delta)$ drops to the negative and the gradient norm $\|\nabla_x \mathcal{L}(x)\|_1$ explodes from near $0$ to above $15$. Paired with the $0\%$ accuracy against PGD-50 and AutoAttack-$L_\infty$ in Table \ref{tab:main_results}, this confirms our view that the model exploits $R(x, \delta)$ to achieve superficial robustness.

\section{Conclusion}

Our experiments provide empirical evidence for the theoretical view of IGR and FGSM as well as the effect of smooth activations on both methods. However, due to limited computing resources, we are not able to completely follow Rodríguez-Muñoz et al. \cite{rodríguezmuñoz2024characterizingmodelrobustnessnatural} and perform the experiments on modern large-scale models and datasets (ResNet-50 on ImageNet). Similarly, we are also unable to train models via multi-step adversarial training with smooth activations to give a direct evidence that IGR and FGSM can indeed achieve comparable robustness to Adversarial Training. It would be interesting to see if the same phenomenon occurs on larger models and datasets and if smooth activations can also improve multi-step adversarial training.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
