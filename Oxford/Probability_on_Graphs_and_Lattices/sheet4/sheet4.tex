\documentclass{article}

% Packages
\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{enumerate}
\usepackage{dsfont}
\usepackage{bbm}

\usetikzlibrary{automata,positioning}

% Document Layout
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in
\linespread{1.1}

% Page Style
\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass:\ \hmwkTitle}
\rhead{Section \hmwkSection, \firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}
\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

% Paragraph Settings
\setlength\parindent{0pt}
\setlength{\parskip}{5pt}

% Section Management
\newcommand{\hmwkSection}{A} % Current section (A, B, or C) - update manually

% Problem Header Management
\newcommand{\enterProblemHeader}[1]{
  \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
  \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
  \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
  \stepcounter{#1}
  \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

% Counters
\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

% Homework Problem Environment
% Optional argument adjusts problem counter for non-sequential problems
\newenvironment{homeworkProblem}[1][-1]{
  \ifnum#1>0
    \setcounter{homeworkProblemCounter}{#1}
  \fi
  \section{Problem \arabic{homeworkProblemCounter}}
  \setcounter{partCounter}{1}
  \enterProblemHeader{homeworkProblemCounter}
}{
  \exitProblemHeader{homeworkProblemCounter}
}

% Assignment Details
\newcommand{\hmwkTitle}{Sheet\ \#4}
\newcommand{\hmwkDueDate}{December 2, 2025}
\newcommand{\hmwkClass}{SC9 Probability on Graphs and Lattices}
\newcommand{\hmwkClassInstructor}{Professor C. Goldschmidt and Professor J. Jorritsma}
\newcommand{\hmwkAuthorName}{\textbf{Ray Tsai}}

% Title Page
\title{
  \vspace{2in}
  \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
  \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 12:00pm}\\
  \vspace{0.1in}\large{\textit{\hmwkClassInstructor}} \\
  \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

% Part Command
\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

% Mathematical Commands
% Algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% Calculus
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}
\newcommand{\dx}{\mathrm{d}x}

% Probability and Statistics
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand*{\prob}{\mathds{P}}
\newcommand*{\E}{\mathds{E}}

% Number Sets
\newcommand*{\Z}{\mathbb{Z}}
\newcommand*{\Q}{\mathbb{Q}}
\newcommand*{\R}{\mathbb{R}}
\newcommand*{\C}{\mathbb{C}}
\newcommand*{\N}{\mathbb{N}}

\begin{document}

\maketitle
\pagebreak

\begin{homeworkProblem}
  \textbf{Some basic properties of the total variation distance.}

  Show that, for two probability measures $\mu, \nu$ on a finite set $S$:
  \begin{enumerate}[(a)]
    \item $|\mu - \nu|_{\text{TV}} = \frac{1}{2}\sum_{x \in S} |\mu(x) - \nu(x)|$;
    \begin{proof}
      Note that for $A \subseteq S$, 
      \[
        \mu(A) - \nu(A) = \sum_{x \in A} \mu(x) - \nu(x).
      \]
      Let $A^+ = \{x \in S : \mu(x) - \nu(x) \geq 0\}$ and $A^- = \{x \in S : \mu(x) - \nu(x) < 0\}$. Then,
      \[
        |\mu - \nu|_{\text{TV}} = \max_{A \subseteq S} |\mu(A) - \nu(A)| = \max(\mu(S^+) - \nu(S^+), \nu(S^-) - \mu(S^-)).
      \]
      But then note that
      \[
        \mu(S^+) + \mu(S^-) = 1 = \nu(S^+) + \nu(S^-) \implies \mu(S^+) - \nu(S^+) = \nu(S^-) - \mu(S^-),
      \]
      so
      \[
        \sum_{x \in S} |\mu(x) - \nu(x)| = (\mu(S^+) - \nu(S^+)) + (\nu(S^-) - \mu(S^-)) = 2\max(\mu(S^+) - \nu(S^+), \nu(S^-) - \mu(S^-)).
      \]
      The reuslt now follows.
    \end{proof}
    \item $|\mu - \nu|_{\text{TV}} = \sum_{x \in S} \max\{\mu(x) - \nu(x), 0\}$;
    \begin{proof}
      By the proof of (a), we have
      \[
        |\mu - \nu|_{\text{TV}} = \sum_{x \in S^+} \mu(x) - \nu(x) = \sum_{x \in S} \max\{\mu(x) - \nu(x), 0\}.
      \]
    \end{proof}
    \item the mapping $(\mu, \nu) \mapsto |\mu - \nu|_{\text{TV}}$ is a distance on the set of all probability measures on $S$;
    \begin{proof}
      It is clear that $|\mu - \mu|_{\text{TV}} = 0$ and $|\mu - \nu|_{\text{TV}} = |\nu - \mu|_{\text{TV}}$. The triangle inequality follows from the fact that
      \begin{align*}
        |\mu - \gamma|_{\text{TV}} 
        &= \frac{1}{2}\sum_{x \in S} |\mu(x) - \nu(x)| \\
        &= \frac{1}{2}\sum_{x \in S} |\mu(x) - \nu(x) + \nu(x) - \gamma(x)| \\
        &\leq \frac{1}{2}\sum_{x \in S} |\mu(x) - \gamma(x)| + \frac{1}{2}\sum_{x \in S} |\nu(x) - \gamma(x)| \\
        &= |\mu - \gamma|_{\text{TV}} + |\nu - \gamma|_{\text{TV}}.
      \end{align*}
    \end{proof}

    \newpage

    \item there exists a coupling $(X^*, Y^*)$ of $\mu$ and $\nu$ such that $\mathbb{P}(X^* \neq Y^*) = |\mu - \nu|_{\text{TV}}$.
    \begin{proof}
      Let $p = \sum_{x \in S} \min(\mu(x), \nu(x))$. By (b),
      \[
        1 - p = \sum_{x \in S} \mu -  \min(\mu(x), \nu(x)) = \sum_{x \in S} \max(\mu(x) - \nu(x), 0) = |\mu - \nu|_{\text{TV}}.
      \]
      We couple $X^*$ and $Y^*$ as follows:
      \begin{enumerate}[1.]
        \item Let $C$ be the bernoulli random variable with parameter $p$.
        \item If $C = 1$, we sample a value $Z$ with distribution
        \[
          \prob(Z = x) = \frac{\min(\mu(x), \nu(x))}{p},
        \]
        and set $X^* = Y^* = Z$.
        \item If $C = 0$, we sample a value $Z_\mu, Z_\nu$ with distribution
        \[
          \prob(Z_\mu = x) = \frac{\mu(x) - \min(\mu(x), \nu(x))}{1 - p}, \quad \prob(Z_\nu = x) = \frac{\nu(x) - \min(\mu(x), \nu(x))}{1 - p}.
        \]
        Evidently,
        \[
          \prob(X^* \neq Y^*) = 1 - p = |\mu - \nu|_{\text{TV}}.
        \]
        We also have that
        \[
          \prob(X^* = x) = p\prob(Z = x) + (1 - p)\prob(Z_\mu = x) = \min(\mu(x), \nu(x)) + \nu(x) - \min(\mu(x), \nu(x)) = \mu(x),
        \]
        \[
          \prob(Y^* = x) = p\prob(Z = x) + (1 - p)\prob(Z_\nu = x) = \min(\mu(x), \nu(x)) + \mu(x) - \min(\mu(x), \nu(x)) = \nu(x).
        \]
      \end{enumerate}
    \end{proof}
  \end{enumerate}
\end{homeworkProblem}

\newpage

\begin{homeworkProblem}
  \textbf{$\rho$ is submultiplicative.}

  Using Q1(d) or otherwise, show that $\rho(t) := \max_{x,y} |\mathbb{P}_{x}^t - \mathbb{P}_{y}^t|_{\text{TV}}$ (defined in Section 4.1) satisfies $\rho(t+s) \leq \rho(t)\rho(s)$ for all $t, s \in \mathbb{N}$.

  \begin{proof}
    For starting states $x, y$, there exists a coupling $(X_t, Y_t)$ such that $X_0 = x$, $Y_0 = y$, and $\prob(X_t \neq Y_t) = |\mathbb{P}^t_{x} - \mathbb{P}^t_{y}|_{\text{TV}}$, by Q1(d). Note that if $X_t = Y_t$, then $X_{t + s} = Y_{t + s}$ and thus
    \[
      |\prob^{t + s}_x - \prob^{t + s}_y| = \prob(X_{t + s} \neq Y_{t + s} \mid X_t = Y_t) = 0
    \]
    On the other hand, Suppose $X_t = u$ and $Y_t = v$ for some $u \neq v$. Then 
    \[
      \prob(X_{t + s} \neq Y_{t + s} \mid X_t = u, Y_t = v) = |\prob^{s}_u - \prob^{s}_v| \leq \rho(s).
    \] 
    It now follows that
    \[
      \rho(t + s) = \prob(X_{t} = Y_t) \cdot 0 + \prob(X_{t} \neq Y_t) \cdot \rho(s) = |\mathbb{P}^t_{x} - \mathbb{P}^t_{y}|_{\text{TV}} \cdot \rho(s) \leq \rho(t)\rho(s).
    \]
  \end{proof}
\end{homeworkProblem}

\newpage

% To change sections, use: \renewcommand{\hmwkSection}{B}
% Example: Uncomment the line below when you start Section B
\renewcommand{\hmwkSection}{B}

\begin{homeworkProblem} 
  \textbf{Lazy random walk on the cycle.}

  Consider a lazy random walk on the $n$-cycle, i.e.\ a Markov chain on the set $[n]$ with transition probabilities $p_{i,i} = \frac{1}{2}$, $p_{i,j} = \frac{1}{4} \mathds{1}_{j \equiv i \pm 1 \pmod n}$.

  \begin{enumerate}[(a)]
    \item Using a coupling and the fact that $\mathbb{E}[T] = k(n-k)$, where $T$ is the time it takes a simple symmetric random walk started at $k$ to hit either $0$ or $n$, show $t_{\text{mix}} \leq n^2$ for this chain.
    \begin{proof}
      Fix two starting vertices $x, y$ with $x \neq y$. Let $(X_t, Y_t)_{t \geq 0}$ be a markov chain with states $\Z/n\Z$. At each step $t$, we choose one of the following four events with equal probability:
      \begin{itemize}
        \item $X_{t + 1} = X_t + 1, \quad Y_{t + 1} = Y_t$
        \item $X_{t + 1} = X_t - 1, \quad Y_{t + 1} = Y_t$
        \item $X_{t + 1} = X_t, \quad Y_{t + 1} = Y_t + 1$
        \item $X_{t + 1} = X_t, \quad Y_{t + 1} = Y_t - 1$
      \end{itemize}
      Note that the marginal distributions of $(X_t)_{t \geq 0}$ and $(Y_t)_{t \geq 0}$ are lazy random walks on $C_n$ with the given rules, so $(X_t, Y_t)$ is a coupling of $(\prob_x^t, \prob_y^t)$. Notice that $(D_t)_{t \neq 0} = (X_t - Y_t)_{t \geq 0}$ is a markov chain on $\Z/n\Z$ with transition probabilities $\prob(D_{t + 1} = k \mid D_t = j) = \frac{1}{4} \mathds{1}_{j \equiv k \pm 1 \pmod n}$. But then $D_t$ is a simple lazy random walk on $\Z/n\Z$. Let $T = \min\{t \geq 0 \mid D_t = 0\}$. Then
      \[
        \max_k \E[T \mid D_0 = k] = \max_k k(n - k) = \frac{n^2}{4}.
      \]
      By the Markov Inequality,
      \[
        \prob(X_t \neq Y_t) = \prob(t < T) \leq \frac{\E[T]}{t} \leq \frac{n^2}{4t}.
      \] 
      It now follows from Lemma 4.3 and Lemma 4.4 that
      \[
        d(t) \leq \rho(t) = \max_{x, y \in \Z/n\Z} |\prob_x^t - \prob_y^t|_{\text{TV}} \leq \max_{x, y \in \Z/n\Z} \prob(X_t \neq Y_t) \leq \frac{n^2}{4t}.
      \]
      Thus, $d(t) \leq 1/4$ when $t \geq n^2$. This completes the proof.
    \end{proof}
    \item Using the Central Limit Theorem or otherwise, find also a lower bound for the mixing of order $n^2$.
    \begin{proof}
      Consider $X_t$ a markov chain on $\Z$ instead of $\Z/n\Z$. By the symmetry of $C_n$, we may assume $X_t$ starts at $0$. Then $X_t = \sum_{i = 1}^t \xi_i$, where $\xi_i$ are i.i.d. random variables with $\prob(\xi_i = 1) = \prob(\xi_i = -1) = \frac{1}{4}$ and $\prob(\xi_i = 0) = \frac{1}{2}$. Thus, $X_i$ has mean $0$ and variance $\frac{t}{2}$. By the Central Limit Theorem, 
      \[
        X_t \approx \mathcal{N}(0, \frac{t}{2}),
      \]
      so $X_t$ has fluctuation of order $\sqrt{t}$. Thus, for $X_t$ to hit $n$ we need $t = \Omega(n^2)$.
    \end{proof}
  \end{enumerate}
\end{homeworkProblem}

\newpage

\begin{homeworkProblem}
  \textbf{Random 3-colouring of the star graph.}

  The \textit{star graph} on $n$ vertices has vertex set $\{1, 2, \dots, n\}$ and edge set $\{(1, v), 2 \leq v \leq n\}$ of size $n-1$. Consider Glauber dynamics for proper 3-colourings of the star graph. (At each step, a vertex $v$ and a colour $c$ are chosen uniformly at random. If no neighbour of $v$ has colour $c$, vertex $v$ is given colour $c$. Otherwise, the colour of $v$ stays unchanged.)

  Fix a time $K$, and let $A_K$ be the event that for some time $t < K$, all the degree-1 vertices $2, 3, \dots, n$ have the same colour at some time $t < K$.

  Show that for the chain started from equilibrium, the probability of $A_K$ is at most $2^{-(n-2)}K$. Hence or otherwise, find a lower bound for the mixing time which grows exponentially in $n$.

  \begin{proof}
    We first note that there are $3 \cdot 2^{n - 1}$ proper 3-colorings of the star graph, and so $\pi(\sigma) = \frac{1}{3 \cdot 2^{n - 1}}$. Let $E_t$ be the event that all the degree-1 vertices $2, 3, \dots, n$ have the same colour at time $t$. Note that there are there are $3 \cdot 2$ ways to two color the star graph, so 
    \[
      \prob(E_t) = \frac{3 \cdot 2}{3 \cdot 2^{n - 1}} = \frac{1}{2^{n - 2}}.
    \]
    Thus by the union bound,
    \[
      \prob(A_k) = \prob\left(\bigcup_{t < K} E_t \right) \leq \sum_{t < K} \prob(E_t) = \frac{K}{2^{n - 2}}
    \]
    Fix $x \in \Omega$. Say $x$ has center color 1. Notice that the center color of $x$ changes after $t$ steps only if event $A_t$ doeshappens. Thus,
    \[
      \prob_x^t(\text{Center} = 1) \geq 1 - \prob(A_t) \geq 1 - \frac{t}{2^{n - 2}}.
    \]
    But then for $n \geq 3$,
    \[
      d(t) \geq |\prob_x^t - \pi|_{\text{TV}} \geq |\prob_x^t(\text{Center} = 1) - \pi(\text{Center} = 1)| \geq 1 - \frac{t}{2^{n - 2}} - \frac{1}{3} = \frac{2}{3} - \frac{t}{2^{n - 2}}.
    \]
    Since $\frac{2}{3} - \frac{t}{2^{n - 2}} \leq 1/4$ for $t \geq 2^{n - 3}$, we have $t_{\text{mix}} \geq 2^{n - 3}$.
  \end{proof}
\end{homeworkProblem}

\newpage

\begin{homeworkProblem}
  \textbf{Card shuffling: top-to-random and random-to-top.} 
  
  Consider a standard deck of 52 cards. A top-to-random shuffle of the deck consists in picking the card on top of the deck and re-inserting it in a uniform random position (among 52 positions, ranging from the top to the bottom one in the deck). A random-to-top shuffle consists in picking a uniform random card among the 52 and putting it on top of the deck.

  \begin{enumerate}[(a)]
    \item Using a coupling, show that $t_{mix}^{RtT}(1/4) \leq 278$ for the Markov Chain on the set of all $52!$ possible orderings of the deck whose steps are independent random-to-top shuffles.
    
    \begin{proof}
      Let $S$ be the set of all $52!$ possible orderings of the deck. Fix $x \in S$ with $x \neq y$. Let $(X_t, Y_t)_{t \geq 0}$ be a pair markov chains on $S$ such that $X_0 = x$ and $Y_0 = y$. At each step $t$, we pick a random card $C_t$ from the 52 cards, and we locate $C_t$ in $X_t$ and $Y_t$ and move them to the top. Notice
      \[
        \prob(X_t \neq Y_t) \leq \prob(\{C_1, \ldots, C_t\} \neq \{1, \ldots, 52\}) = \prob(T > t),
      \]
      where $T = \min\{t \geq 0 \mid \{C_1, \ldots, C_t\} = \{1, \ldots, 52\}\}$. By Lemma 4.3, Lemma 4.4, and the calculation in the proof of Section 4.2,
      \[
        d(t) \leq\prob(T > t(\log 4, 52)) \leq \frac{1}{4}
      \]
      when $t(1/4, 52) = \lceil 52\log 52 + 52\log 4 \rceil = \lceil 52\log 208 \rceil \leq 278$. Thus, $t_{\text{mix}}^{RtT}(1/4) \leq 278$.
    \end{proof}
    
    \item Let $(P^{TtR})_x^t$ and $(P^{RtT})_x^t$ be the distributions of the top-to-random and random-to-top chains started at $x$, respectively, after $t$ steps; given a permutation $\sigma \in S_{52}$, let $\sigma(x)$ be the deck obtained by applying the permutation $\sigma$ to $x$. Show that
    \[
    (P^{TtR})_x^t(\sigma(x)) = (P^{RtT})_x^t(\sigma^{-1}(x))
    \]
    and deduce that $|(P^{TtR})_x^t - \pi|_{\text{TV}} = |(P^{RtT})_x^t - \pi|_{\text{TV}}$, where $\pi$ is the uniform distribution on $S_{52}$.

    \begin{proof}
      Let $S_{TtR} = \{(1, i) : i \in [52]\}$ and $S_{RtT} = \{(i, 1) : i \in [52]\}$. $P^{TtR}$ is a measure on the sample space $S_{TtR}$ and $P^{RtT}$ is a measure on the sample space $S_{RtT}$. Since $S_{RtT} = \{g^{-1} : g \in S_{TtR}\}$, we have
      \[
        P^{TtR}(g) = P^{RtT}(g^{-1}).
      \]
      But then each permutation $\sigma \in S_{52}$ may be written as a product of transpositions of the form $(1, i)$. Thus,
      \[
      (P^{TtR})_x^t(\sigma(x)) = (P^{RtT})_x^t(\sigma^{-1}(x)).
      \]
      But then by Q1(a),
      \begin{align*}
        |(P^{TtR})_x^t - \pi|_{TV} 
        &= \frac{1}{2}\sum_{\sigma \in S_{52}} \left|(P^{TtR})_x^t(\sigma(x)) - \frac{1}{52!}\right| \\
        &= \frac{1}{2}\sum_{\sigma \in S_{52}} \left|(P^{RtT})_x^t(\sigma^{-1}(x)) - \frac{1}{52!}\right| \\
        &= \frac{1}{2}\sum_{\sigma \in S_{52}} \left|(P^{RtT})_x^t(\sigma(x)) - \frac{1}{52!}\right| \\
        &= |(P^{RtT})_x^t - \pi|_{TV}.
      \end{align*}
    \end{proof}

    \item Show that the mixing times of the top-to-random chain and the random-to-top chain are equal and provide an upper bound for the mixing time $t_{mix}^{TtR}$ of the chain of top-to-random shuffles for a deck with $n$ cards.
    \begin{proof}
      Let $d^{TtR}$ and $d^{RtT}$ be the mixing times of the top-to-random chain and the random-to-top chain, respectively. Since $|(P^{TtR})_x^t - \pi|_{TV} = |(P^{RtT})_x^t - \pi|_{TV}$,
      \[
        d^{TtR}(t) = \max_{x \in S_52} |(P^{TtR})_x^t - \pi|_{TV} = \max_{x \in S_52} |(P^{RtT})_x^t - \pi|_{TV} = d^{RtT}(t),
      \]
      and thus by (a),
      \[
        t_{mix}^{TtR} = t_{mix}^{RtT} \leq 278.
      \]
    \end{proof}
  \end{enumerate}
\end{homeworkProblem}

\newpage

\begin{homeworkProblem}
  \textbf{The greasy ladder.}

  A frog jumps on a ladder with $n$ rungs. The ground is labelled $0$ and the rungs are labelled $1, 2, \dots, n$. At each time, with probability $1/2$ the frog falls back to level $0$, and otherwise $1/2$ the frog jumps up one level (unless he is already at level $n$, in which case he remains there).
    
  That is, $p_{i,i+1} = 1/2$ for $0 \leq i \leq n-1$, $p_{i,0} = 1/2$ for all $i$, and $p_{n,n} = 1/2$.

  \begin{enumerate}[(a)]
    \item Find the mixing time of the chain.
    \begin{proof}
      Let $x, y \in [n]$. Let $(X_t, Y_t)_{t \geq 0}$ be a pair markov chains on $[n]$ such that $X_0 = x$ and $Y_0 = y$ and $x \neq y$. Let $(Z_t)_{t \geq 0}$ be a bernoulli random variable with parameter $1/2$. At each step $t$, if $Z_t = 1$, then $X_{t + 1} = \min(X_t + 1, n)$ and $Y_{t + 1} = \min(Y_t + 1, n)$. If $Z_t = 0$, then $X_{t + 1} = Y_{t + 1} = 0$. Then $(X_t, Y_t)$ is a coupling of $(\prob_x^t, \prob_y^t)$. Notice
      \[
        \prob(X_t \neq Y_t) \leq \prob(Z_0 = Z_1 = \cdots = Z_{t - 1} = 1) = (1/2)^t.
      \]
      Thus by Lemma 4.3 and Lemma 4.4,
      \[
        d(t) \leq \rho(t) = \max_{x, y \in [n]} |\prob_x^t - \prob_y^t|_{\text{TV}} \leq \max_{x, y \in [n]} \prob(X_t \neq Y_t) \leq (1/2)^t.
      \]
      Since $(1/2)^t \leq 1/4$ for $t \geq 2$, we have $t_{mix} \leq 2$.
    \end{proof}
    \item In Q5, a Markov chain and its time reversal were shown to have the same mixing time. This is true for the class of ``random walks on groups'' (for which the shuffle in Q5 is an example). However, it is not true in general! Find the transition probabilities of the time-reversal of the frog chain, and find its mixing time.
    \begin{proof}
      We first note that the stationary distribution of the frog chain is
      \[
        \pi(k) = \begin{cases}
          \frac{1}{2^{k + 1}} & \text{if } 0 \leq k < n \\
          \frac{1}{2^{n}} & \text{if } k = n
        \end{cases}.
      \]
      Let $p^*_{i, j}$ denote the transition probability of the time-reversal of the frog chain. Then
      \[
        p^*_{i, j} = \prob(X_t = j \mid X_{t + 1} = i) = \frac{\prob(X_{t + 1} = i \mid X_t = j) \prob(X_t = j)}{\prob(X_{t + 1} = i)} = \frac{p_{j, i}\pi(j)}{\pi(i)}. 
      \]
      Thus,
      \[
        p^*_{0, j} = \pi(j), \quad p^*_{n, n} = p^*_{n, n - 1} = \frac{1}{2}, \quad p^*_{i, i - 1} = 1, \text{ for } 0 < i < n.
      \]
      Let $x, y \in [n]$ with $x \neq y$. Let $(X_t, Y_t)_{t \geq 0}$ be a pair markov chains on $[n]$ such that $X_0 = x$ and $Y_0 = y$.
      
      Let $T$ be the mixing time of the reverse frog chain $(X_t)_{t \geq 0}$. Since the transition probabilities of the reverse frog chain starting from $0$ follows the stationary distribution, the chain the mixed one step after hitting $0$ for the first time. Thus
      \[
        d(t) \leq \prob(T > t) \cdot 1 + \prob(T \leq t) \cdot 0 = \prob(T > t).
      \]
      Notice that once the chain hits $k$ for some $0 < k < n$, then it will hit $0$ in exactly $k$ steps. Thus the worst case occurs when $X_0 = n$, which takes $T = X + (n - 1) + 1 = X + n$ steps to mix, where $X$ is the steps needed to exit $n$. Thus by Markov's Inequality,
      \[
        d(t) \leq \prob(T > t) = \prob(X > t - n) \leq \frac{\E[X]}{t - n} = \frac{2}{t - n}.
      \]
      Since $d(t) \leq 1/4$ for $t \geq n + 8$, we have $t_{mix} \leq n + 8$.
    \end{proof}
  \end{enumerate}
\end{homeworkProblem}

\end{document}