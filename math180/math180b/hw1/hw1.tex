\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{dsfont}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass:\ \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}
\setlength{\parskip}{5pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Homework\ \#1}
\newcommand{\hmwkDueDate}{Jan 19, 2023}
\newcommand{\hmwkClass}{MATH 180B}
\newcommand{\hmwkClassInstructor}{Professor Carfagnini}
\newcommand{\hmwkAuthorName}{\textbf{Ray Tsai}}
\newcommand{\hmwkPID}{A16848188}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate\ at 23:59pm}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor}} \\
    \vspace{3in}
}

\author{
  \hmwkAuthorName \\
  \vspace{0.1in}\small\hmwkPID
}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}
\newcommand*{\Z}{\mathbb{Z}}
\newcommand*{\Q}{\mathbb{Q}}
\newcommand*{\R}{\mathbb{R}}
\newcommand*{\C}{\mathbb{C}}
\newcommand*{\N}{\mathcal{N}}
\newcommand*{\p}{\mathds{P}}
\newcommand*{\E}{\mathds{E}}

\begin{document}

\maketitle

\pagebreak

\begin{homeworkProblem}
  Let $U, V,$ and $W$ be independent random variables with equal variance $\sigma^2$. Define $X = U
  + W$ and $Y = V - W$. Find the covariance between $X$ and $Y$.

  \begin{proof}
    \begin{align*}
      Cov(X, Y) 
      &= \E[XY] - \E[X]\E[Y] \\
      &= \E[(U + W)(V - W)] - \E[U + W]\E[V - W] \\
      &= \E[UV + WV - UW - W^2] - (\E[U] + \E[W])(\E[V] - \E[W]) \\
      &= \E[U]\E[V] + \E[W]\E[V] - \E[U]\E[W] - \E[W^2] - \E[U]\E[V] - \E[W]\E[V] + \E[U]\E[W] + \E[W^2] \\
      &= 0.
    \end{align*}
  \end{proof}
\end{homeworkProblem}

\newpage

\begin{homeworkProblem}
  Let $X$ and $Y$ be independent binomial random variables having parameters $(N, p)$ and $(M, p)$,
  respectively. Let $Z = X + Y$.

  \begin{enumerate}[(a)]
    \item Argue that $Z$ has a binomial distribution with parameters $(N + M, p)$ by writing $X$ and
    $Y$ as appropriate sums of Bernoulli random varaibles.
    \begin{proof}
      Since $\p(X = i) = {N \choose i}p^{i}(1 - p)^{N - i}$ and $\p(Y = i) = {M \choose i}p^{i}(1 -
      p)^{M - i}$, $X$ is the sum of $N$ indicators and $Y$ is the sum of $M$ indicators. Hence, we
      have $Z = X + Y$ as the sum of $M + N$ indicators.
    \end{proof}

    \item Validate the results in (a) by evaluating the necessary convolution.
    \begin{proof}
      Since 
      \begin{align*}
        \p(Z = k) 
        &= \sum_{i = 0}^k \p(X = i)\p(Y = k - i) \\
        &= \sum_{i = 0}^k {N \choose i}p^{i}(1 - p)^{N - i}{M \choose k - i}p^{k - i}(1 - p)^{M - (k - i)} \\
        &= p^{k}(1 - p)^{(M + N) - k}\sum_{i = 0}^k {N \choose i}{M \choose k - i} \\
        &= {M + N \choose k}p^{k}(1 - p)^{(M + N) - k},
      \end{align*}
      $Z$ has a binomial distribution with parameters $(N + M, p)$.
    \end{proof}
  \end{enumerate}
\end{homeworkProblem}

\newpage

\begin{homeworkProblem}
  Let $X$ be a random variable. Recall that the moment generating function (or MGF for short)
  $M_X(t)$ of $X$ is the function $M_X : \R \rightarrow \R \cup \{\infty\}$ defined by $t \mapsto
  \E[e^{tX}]$. Now suppose that $X \sim Gamma(\alpha, \lambda)$, where $\alpha, \lambda > 0$.

  \begin{enumerate}
    \item Prove that
    \[
      M_X(t) = \begin{cases}
        \left(\frac{\lambda}{\lambda - t}\right)^{\alpha} & \text{if } t < \lambda; \\
        \infty & \text{if } t \geq \lambda.
      \end{cases}
    \]
    \begin{proof}
      Let $u = (\lambda - t)x$. We know $du = (\lambda - t)dx$. Then,
      \begin{align*}
        M_X(t)
        &= \int_0^{\infty} \frac{\lambda}{\Gamma(\alpha)} (\lambda x)^{\alpha - 1} e^{-\lambda x}e^{tx} dx \\
        &= \frac{\lambda^{\alpha}}{\Gamma(\alpha)} \int_0^{\infty}  x^{\alpha - 1} e^{(t - \lambda) x} dx \\
        &= \frac{\lambda^{\alpha}}{\Gamma(\alpha)} \int_0^{\infty} \left(\frac{u}{\lambda - t}\right)^{\alpha - 1}e^{-u} \frac{du}{\lambda - t} \\
        &= \left(\frac{\lambda}{\lambda - t}\right)^{\alpha} \frac{\int_0^{\infty} u^{\alpha - 1}e^{-u} du}{\Gamma(\alpha)}.
      \end{align*}
      If $t \geq \lambda$, we get $-u > 0$, so the integral $\int_0^{\infty} u^{\alpha - 1}e^{-u}
      du$ would approach infinity. Otherwise, $\int_0^{\infty} u^{\alpha - 1}e^{-u} du =
      \Gamma(\alpha)$, and we get $M_{X}(t) = \left(\frac{\lambda}{\lambda - t}\right)^{\alpha}$.
    \end{proof}

    \item Recall that the MGF contains the information of the moments. In particular, if $m_l(X$) is
    the $l$-th moment of $X$, then $M^{(l)}_X(0) = m_l(X)$, where $M^{(l)}_X$ denotes the $l$-th
    derivative of $M_X$. Use this to compute the mean and variance of $X$.

    \begin{proof}
      Note that 
      \[
        M_X(t) = \E[e^{tX}] = \sum_{k = 0}^{\infty} \E\left[\frac{(tX)^k}{k!}\right] = \sum_{k = 0}^{\infty} \frac{t^k}{k!} \E[X^k].
      \]
      Since all the terms after the first one in $M^{(l)}_X$ is multiplied by a power of $t$, only
      the first term remains when $t$ is set to 0, and thus $m_l(X) = M^{(l)}_X(0) = \E[X^l]$. To
      calculate the mean $\mu$ and variance $\sigma^2$ of $X$, we only need to calculate $\E[X]$ and
      $\E[X^2]$, namely $m_1(X)$ and $m_2(X)$. Since $t < \lambda$, 
      \[
        m_1(X) = \left.\frac{\alpha\lambda^{\alpha}}{(\lambda - t)^{\alpha + 1}}\right|_{t = 0} = \frac{\alpha}{\lambda}
      \]
      \[
        m_2(X) =  \left.\frac{\alpha(\alpha + 1)\lambda^{\alpha}}{(\lambda - t)^{\alpha + 2}}\right|_{t = 0} = \frac{\alpha(\alpha + 1)}{\lambda^2},
      \]
      and thus $\mu = m_1(X) = \frac{\alpha}{\lambda}$ and $\sigma^2 = m_2(X) - m_1(X)^2 =
      \frac{\alpha(\alpha + 1)}{\lambda^2} - \left(\frac{\alpha}{\lambda}\right)^2 =\frac{\alpha}{\lambda^2}$.
    \end{proof}
  \end{enumerate}
\end{homeworkProblem}

\newpage

\begin{homeworkProblem}
  Suppose that $(X_1, X_2)$ has the bivariate normal distribution with marginals $X_1 \sim \N(\mu_1,
  \sigma^2_1)$ and $X_2 \sim \N(\mu_2, \sigma^2_2)$ and correlation $Corr(X_1, X_2) = \rho$. Let $Y_1
  = 2X_1 + X_2$ and $Y_2 = X_1 - X_2$. Determine the distribution of the random vector $(Y_1,
  Y_2)$.

  \begin{proof}
    Let $X = (X_1, X_2)^T$, $Y = (Y_1, Y_2)^T$. Note that $Corr(X_1, X_2) = \frac{Cov(X_1,
    X_2)}{\sigma_1\sigma_2}$, so $Cov(X_1, X_2) = Cov(X_2, X_1) = \rho\sigma_1\sigma_2$. Thus, we
    get the covariance matrix of $X$, which is 

    \[
      \Sigma_X = \E[(X - \E[X])(X - \E[X])^T] = \begin{bmatrix} 
        \sigma_1^2 & \rho\sigma_1\sigma_2 \\
        \rho\sigma_1\sigma_2 & \sigma_2^2 \\
      \end{bmatrix}.
    \]
    
    Let $A = \begin{bmatrix} 2 & 1 \\
      1 & -1 \\
    \end{bmatrix}$. Since $Y = A^TX$ and $X$ is a bivariate Gaussgian random variable, we get
    $$\mu_Y = \E[Y] = A^T\E[X] = A^T(\mu_1, \mu_2)^T = (2\mu_1 + \mu_2, \mu_1 - \mu_2)^T,$$ and 
    \begin{align*}
      \Sigma_Y 
      &= \E[(Y - \E[Y])(Y - \E[Y])^T] \\
      &= \E[(A^TX - A^T\E[X])(A^TX - A^T\E[X])^T] \\
      &= \E[A^T(X - E[X])(X - \E[X])A] \\
      &= A^T\E[(X - \E[X])(X - \E[X])^T]A \\
      &= A^T\Sigma_XA \\
      &= \begin{bmatrix}
        4\sigma_1^2 + 5\rho\sigma_1\sigma_2 + \sigma_2^2 & 2\sigma_1^2 - \rho\sigma_1\sigma_2 - \sigma_2^2 \\
        2\sigma_1^2 - 3\rho\sigma_1\sigma_2 - \sigma_2^2 & \sigma_1^2 - 2\rho\sigma_1\sigma_2 + \sigma_2^2
      \end{bmatrix}.
    \end{align*}
    Therefore, $Y \sim \N(\mu_Y, \Sigma_Y)$.
  \end{proof}
\end{homeworkProblem}

\newpage

\begin{homeworkProblem}
  Let $X \sim \text{Unif}[-1, 1]$. Consider the functions $g, h: [-1, 1] \rightarrow [-1, 1]$ given by
  \[
    g(x) = \begin{cases}
      1 - x & \text{if } x \in [0, 1]; \\
      x & \text{if } x \in [-1, 0),
    \end{cases}
  \]
  and
  \[
    h(x) = \begin{cases}
      x & \text{if } x \in [0, 1]; \\
      -(x + 1) & \text{if } x \in [-1, 0).
    \end{cases}
  \]
  \begin{enumerate}[(a)]
    \item Prove that $Y = g(X)$ and $Z = h(X)$ are both uniform $Y, Z \sim$ Unif$[-1, 1]$.
    \begin{proof}
      Let $k \in [-1, 1]$, and let $\alpha = \p(X = 0)$. Note that $\p(X = x) = \alpha$, for all $x
      \in [-1, 1]$. Suppose that $k \geq 0$. Then, $\p(Y = k) = \p(X = 1 - k) = \alpha$ and $\p(Z =
      k) = \p(X = k) = \alpha$. Suppose that $k < 0$. Then, $\p(Y = k) = \p(X = k) = \alpha$ and
      $\p(Z = k) = \p(X = -(k + 1)) = \alpha$. Since $\p(Y = k) = \p(Z = k) = \alpha$ for all $l \in
      [-1, 1]$, $Y, Z \sim$ Unif$[-1, 1]$.
    \end{proof}
    \item Prove that $Cov(X, Y) = Cov(X, Z)$.
    \begin{proof}
      Since 
      \[
        Cov(X, Y) = \E[XY] - \E[X]\E[Y] = \E[XY] = \alpha\left(\int_{-1}^0 x^2 dx + \int^1_0 x(1 - x)dx\right) = \frac{\alpha}{2}
      \]
      and
      \[
        Cov(X, Z) = \E[XZ] - \E[X]\E[Z] = \E[XZ] = \alpha\left(\int_{-1}^0 -(x + 1)x dx + \int^1_0 x^2 dx\right) = \frac{\alpha}{2},
      \]
      we get $Cov(X, Y) = Cov(X, Z)$.
    \end{proof}
    \item Prove that the random vectors $(X, Y)$ and $(X, Z)$ do not have the same joint
    distribution. This can be done by finding a subset $B \subset \R^2$ such that
    \[
      \p((X, Y) \in B) \neq \p((X, Z) \in B).
    \]
    \begin{proof}
      Consider $B = \{(x, x) \, | \, x \in [0, 1]\}$. Since $\p((X, Y) \in B) = 0 \neq \frac{1}{2} =
      \p((X, Z) \in B)$, $(X, Y)$ and $(X, Z)$ do not have the same joint distribution.
    \end{proof}
  \end{enumerate}
\end{homeworkProblem}
\end{document}